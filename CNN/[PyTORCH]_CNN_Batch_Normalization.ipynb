{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FLyjTT82Qdy",
        "outputId": "3e1816f8-809c-4b5b-947f-eba717bf0746"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7caca6b650>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "RANDOM_SEED = 1\n",
        "torch.manual_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 10\n",
        "n_epochs = 50\n",
        "batch_size = 256\n",
        "learning_rate = 1e-4"
      ],
      "metadata": {
        "id": "Qs23HU48GALx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super(CNNModel, self).__init__()\n",
        "    self.conv_layer1 = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, 3, stride=1, padding='same'), \n",
        "        nn.ReLU(), \n",
        "        nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.conv_layer2 = nn.Sequential(\n",
        "        nn.Conv2d(64, 64, 3, stride=1, padding='same'), \n",
        "        nn.ReLU(), \n",
        "        nn.BatchNorm2d(64)\n",
        "    )\n",
        "    self.conv_layer3 = nn.Sequential(\n",
        "        nn.Conv2d(64, 64, 3, stride=1, padding='same'), \n",
        "        nn.ReLU(), \n",
        "        nn.BatchNorm2d(64), \n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.conv_layer4 = nn.Sequential(\n",
        "        nn.Conv2d(64, 128, 3, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.conv_layer5 = nn.Sequential(\n",
        "        nn.Conv2d(128, 128, 3, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(128)\n",
        "    )\n",
        "    self.conv_layer6 = nn.Sequential(\n",
        "        nn.Conv2d(128, 128, 3, stride=1, padding='same'), \n",
        "        nn.ReLU(), \n",
        "        nn.BatchNorm2d(128), \n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.conv_layer7 = nn.Sequential(\n",
        "        nn.Conv2d(128, 256, 3, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.conv_layer8 = nn.Sequential(\n",
        "        nn.Conv2d(256, 256, 3, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(256)\n",
        "    )\n",
        "    self.conv_layer9 = nn.Sequential(\n",
        "        nn.Conv2d(256, 256, 3, stride=1, padding='same'), \n",
        "        nn.ReLU(), \n",
        "        nn.BatchNorm2d(256), \n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.conv_layer10 = nn.Sequential(\n",
        "        nn.Conv2d(256, 512, 3, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.conv_layer11 = nn.Sequential(\n",
        "        nn.Conv2d(512, 512, 3, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512)\n",
        "    )\n",
        "    self.conv_layer12 = nn.Sequential(\n",
        "        nn.Conv2d(512, 512, 3, stride=1, padding='same'), \n",
        "        nn.ReLU(), \n",
        "        nn.BatchNorm2d(512), \n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc_layer1 = nn.Sequential(\n",
        "        nn.Linear(512 * 2 * 2, 512), \n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.fc_layer2 = nn.Linear(512, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_layer1(x)\n",
        "    x = self.conv_layer2(x)\n",
        "    x = self.conv_layer3(x)\n",
        "    x = self.conv_layer4(x)\n",
        "    x = self.conv_layer5(x)\n",
        "    x = self.conv_layer6(x)\n",
        "    x = self.conv_layer7(x)\n",
        "    x = self.conv_layer8(x)\n",
        "    x = self.conv_layer9(x)\n",
        "    x = self.conv_layer10(x)\n",
        "    x = self.conv_layer11(x)\n",
        "    x = self.conv_layer12(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc_layer1(x)\n",
        "    outputs = self.fc_layer2(x)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device: {}'.format(device))\n",
        "model = CNNModel(n_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rqKIsMdDpbg",
        "outputId": "2eeeb547-94c6-424b-e9b3-0b0e9523bd2e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNModel(\n",
              "  (conv_layer1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_layer2): Sequential(\n",
              "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_layer3): Sequential(\n",
              "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_layer4): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_layer5): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_layer6): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_layer7): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_layer8): Sequential(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_layer9): Sequential(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_layer10): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_layer11): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_layer12): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc_layer1): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (fc_layer2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "val_set = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size,\n",
        "                              shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(val_set, batch_size=batch_size, \n",
        "                            shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL_SbeccGGP6",
        "outputId": "d2f631ea-f6cd-4010-9a3d-6cb61e989de0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "NzBa4msqGyuR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_losses = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "training_steps = len(train_dataloader)\n",
        "val_steps = len(val_dataloader)"
      ],
      "metadata": {
        "id": "nUJEsBXhHGr3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('START TRAINING .... \\n')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_training_loss = 0\n",
        "\n",
        "  # TRAINING\n",
        "  for idx, (X_train, y_train) in enumerate(train_dataloader):\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    total_training_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  training_loss = total_training_loss/training_steps\n",
        "  training_losses.append(training_loss)\n",
        "  print('EPOCH: [{}/{}], TRAINING LOSS: {}'.format(epoch + 1, n_epochs, training_loss))\n",
        "\n",
        "  # VALIDATING\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    n_correct_preds = 0\n",
        "    n_val_samples = 0\n",
        "    total_val_loss = 0\n",
        "    for idx, (X_val, y_val) in enumerate(val_dataloader):\n",
        "      X_val = X_val.to(device)\n",
        "      y_val = y_val.to(device)\n",
        "\n",
        "      y_pred = model(X_val)\n",
        "      loss = criterion(y_pred, y_val)\n",
        "      total_val_loss += loss.item()\n",
        "\n",
        "      y_pred_max, y_pred_max_idx = torch.max(y_pred, 1)\n",
        "      n_val_samples += y_pred.size(0)\n",
        "      n_correct_preds += (y_val == y_pred_max_idx).sum().item()\n",
        "\n",
        "    val_acc = n_correct_preds / n_val_samples\n",
        "    val_loss = total_val_loss / val_steps\n",
        "    val_accs.append(val_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    print('VAL LOSS: {}, VAL ACCURACY: {}'.format(val_loss, val_acc))\n",
        "\n",
        "print('DONE TRAINING!!!!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSmmpDg4HLlR",
        "outputId": "fd74ce88-c65f-46ee-d5bc-de6146ae2521"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START TRAINING .... \n",
            "\n",
            "EPOCH: [1/50], TRAINING LOSS: 2.387894652327713\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [2/50], TRAINING LOSS: 2.3893058275689882\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [3/50], TRAINING LOSS: 2.3894030938343125\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [4/50], TRAINING LOSS: 2.38919982496573\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [5/50], TRAINING LOSS: 2.389516195472406\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [6/50], TRAINING LOSS: 2.3895125364770693\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [7/50], TRAINING LOSS: 2.389408620036378\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [8/50], TRAINING LOSS: 2.38928038611704\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [9/50], TRAINING LOSS: 2.3894984478853187\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [10/50], TRAINING LOSS: 2.389378676609117\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [11/50], TRAINING LOSS: 2.3893368390141703\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [12/50], TRAINING LOSS: 2.389310139782575\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [13/50], TRAINING LOSS: 2.3891373751114826\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [14/50], TRAINING LOSS: 2.389215717510301\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [15/50], TRAINING LOSS: 2.3889720123641345\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [16/50], TRAINING LOSS: 2.3893983899330604\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [17/50], TRAINING LOSS: 2.389860885483878\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [18/50], TRAINING LOSS: 2.3895019609100965\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [19/50], TRAINING LOSS: 2.3893459731218765\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [20/50], TRAINING LOSS: 2.3895473468060398\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [21/50], TRAINING LOSS: 2.3891692611635946\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [22/50], TRAINING LOSS: 2.3892386190745296\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [23/50], TRAINING LOSS: 2.38941844020571\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [24/50], TRAINING LOSS: 2.3894859296934947\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [25/50], TRAINING LOSS: 2.3892757977758134\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [26/50], TRAINING LOSS: 2.389195211079656\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [27/50], TRAINING LOSS: 2.3891884307472075\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [28/50], TRAINING LOSS: 2.389505630853225\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [29/50], TRAINING LOSS: 2.389024184674633\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [30/50], TRAINING LOSS: 2.38938891157812\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [31/50], TRAINING LOSS: 2.3895395629260006\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [32/50], TRAINING LOSS: 2.3894946879270127\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [33/50], TRAINING LOSS: 2.3892788801874434\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [34/50], TRAINING LOSS: 2.3895272393615876\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [35/50], TRAINING LOSS: 2.3892475050322863\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [36/50], TRAINING LOSS: 2.3895994534297866\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [37/50], TRAINING LOSS: 2.389047844069345\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [38/50], TRAINING LOSS: 2.3891843241088244\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [39/50], TRAINING LOSS: 2.3893818733643513\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [40/50], TRAINING LOSS: 2.3893885211068757\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [41/50], TRAINING LOSS: 2.3893672982040717\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [42/50], TRAINING LOSS: 2.389301632131849\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [43/50], TRAINING LOSS: 2.389583306653159\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [44/50], TRAINING LOSS: 2.389259749529313\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [45/50], TRAINING LOSS: 2.3895196513253816\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [46/50], TRAINING LOSS: 2.389242958049385\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [47/50], TRAINING LOSS: 2.3894320738558865\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [48/50], TRAINING LOSS: 2.389454314903337\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [49/50], TRAINING LOSS: 2.3894945906133067\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "EPOCH: [50/50], TRAINING LOSS: 2.3895234137165304\n",
            "VAL LOSS: 2.3847280740737915, VAL ACCURACY: 0.0998\n",
            "DONE TRAINING!!!!!\n"
          ]
        }
      ]
    }
  ]
}